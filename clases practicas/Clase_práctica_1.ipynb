{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase práctica 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clase Práctica 1: Clasificación 📈\n",
        "\n",
        "\n",
        "----------------------------------"
      ],
      "metadata": {
        "id": "6GAvF9Liisd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el aprendizaje de máquinas, la tarea de clasificación consiste en predecir una variable discreta (perteneciente a un conjunto finito de opciones) a partir de una observación, es decir, un conjunto de características. \n",
        "\n",
        "La principal diferencia entre problemas de clasificación y regresión, es que en estos últimos los modelos predicen una variable continua (conjunto infinito de posibilidades) a partir de los datos entrada. Así, un ejemplo de un problema de regresión sería predecir el precio de una casa a partir de las características como el tamaño, ubicación, material, etc. De lo contrario, un ejemplo de clasificación sería predecir si el contenido de un correo corresponde a un spam o no. En este caso, como el número de respuestas posibles es igual a 2 (finito), se dice que es un problema de clasificación binaria. \n",
        "\n",
        "## Objetivos de la clase 📚\n",
        "\n",
        "El objetivo principal de esta primera clase práctica es realizar una introducción a la tarea de clasificación en el aprendizaje de máquinas. Para esto, primero implementaremos varios modelos de clasificación para poder relacionar las características fisiológicas de pacientes con la presencia de una enfermedad cardiovascular 🩺. \n",
        "\n",
        "Las principales librerías que vamos a utilizar son las siguientes:\n",
        "\n",
        "- Pandas 🐼: Manejo y análisis de estructuras de datos.\n",
        "- Scikit-learn: Librería para el aprendizaje de máquina, contiene no sólo modelos clásicos sino que también una serie de funciones que nos permitirán procesar mejor los datos de entrada.\n",
        "- Numpy: Manejo de operaciones matemáticas.\n",
        "- Matplotlib y Seaborn: Generación de visualizaciones sobre los datos.\n",
        "\n",
        "## Metodología\n",
        "\n",
        "La mayoría de los problemas que se resuelven con aprendizaje de máquinas pueden ser abordados siguiendo 3 pasos principales:\n",
        "\n",
        "1. Exploración de datos.\n",
        "2. Selección de modelos.\n",
        "3. Comparación y análisis de resultados.\n",
        "\n",
        "A pesar de que en aplicaciones reales existen muchos más pasos intermedios, como el estudio del problema a nivel de negocio y la puesta en producción, estos pasos serán suficientes para abordar los distintos problemas que veremos en este curso.\n",
        "\n",
        "Así que empecemos! 🥳\n"
      ],
      "metadata": {
        "id": "ka41Y9Ciisyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Clasificación**"
      ],
      "metadata": {
        "id": "ZqfjgV1Deuyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploración de datos 🧐**"
      ],
      "metadata": {
        "id": "KRLw4Z42m2kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la mayoría de los casos, cuando se resuelven problemas con aprendizaje de máquinas solemos trabajar con datos almacenados en formatos de tablas, con extensiones tales como *csv*, *tsv*, *xlsx* o incluso en algunos casos *txt* (más común al trabajar con texto). \n",
        "\n",
        "*Pandas* 🐼 es una librería de Python especializada en el manejo de estructuras de datos, la cual nos permite poder cargar, procesar y analizar datos tabulados de manera sencilla. Además, nos entrega muchas funciones que en conjunto con otros librerías de visualización como *Matplotlib* o *Seaborn* nos permite obtener excelentes visualizaciones sobre los datos."
      ],
      "metadata": {
        "id": "Jx90SmY_n5YZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importar las librerías**"
      ],
      "metadata": {
        "id": "fO3ZlyjwmyBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns "
      ],
      "metadata": {
        "id": "LwdZuCQ6aEjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lectura del dataset**"
      ],
      "metadata": {
        "id": "uDcwDmSTm2ZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la función `read_csv` podemos cargar datasets que se encuentren en formato csv. Es importante que el nombre de la variable sea lo más simple posible, ya que será utilizado frecuentemente."
      ],
      "metadata": {
        "id": "JEbr1fQ-nZyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/fvillena/biocompu/2022/data/cardiovascular_diseases.csv\")"
      ],
      "metadata": {
        "id": "dOMFr3b-n6Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función `head` nos muestra las primeras 5 filas del dataset"
      ],
      "metadata": {
        "id": "lxV1Y0JzpBW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Yy8db6QLo_ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función `sample(x)` nos entrega x ejemplos del datasets al azar"
      ],
      "metadata": {
        "id": "CIXoza3Mp5Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(3)"
      ],
      "metadata": {
        "id": "G9z2xYlWpkfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función `tail` nos muestra las últimas filas del dataset"
      ],
      "metadata": {
        "id": "2TG4tFHKp-FI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "c37lrSoSqBNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En aprendizaje de máquinas las filas de un dataset son más conocidas como *instancias*, mientras que las columnas son nuestros *atributos* o *features*. Para acceder al número de instancias y de features utilizamos el atributo `shape`. El primer valor corresponde al número de instancia, mientras que el segundo es el número de atributos."
      ],
      "metadata": {
        "id": "S5T3OHVWqHUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape "
      ],
      "metadata": {
        "id": "Ro-c8oBjq0Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resultado de la celda anterior nos muestra que hay un total de 301 instancias, donde cada una tiene un total de 14 atributos. Para acceder al nombre de cada uno de estos atributos, utilizamos el comando `columns`."
      ],
      "metadata": {
        "id": "aaEGqmEFq_EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "dYe-2dTBrHNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Según el resultado anterior, tenemos los siguientes datos del paciente: edad, sexo, tipo de dolor de pecho, colesterol, nivel alto de azucar en sangre en ayudo, resultados electrocardiográficos en reposo, frecuencia cardiaca máxica alcanzada, angina inducida por el ejercicio, depresión st por ejercicio, entre otras."
      ],
      "metadata": {
        "id": "XeYWJrIiDNc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos información más detallada acerca de estos atributos podemos utilizar la función `info`, la cual nos entrega el nombre, la cantidad de ejemplos no nulos y el tipo de dato de cada columna."
      ],
      "metadata": {
        "id": "gbZKXAW-rSO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "N3DOn9pYrW9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos obtener estadística acerca de cada una de nuestras columnas utilizamos la función `describe`. Esta función nos entrega la cantidad de datos, el promedio, desviación estandar, mínimo, máximo y cuartiles."
      ],
      "metadata": {
        "id": "Ok8HMFqPsY73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "UMWeVjRcsYVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un paso fundamental en la exploración de datos es ver que no hayan valores nulos. Para revisar esto de manera rápida, utilizamos la función `isnull`. Esta función analiza celda por celda si existe un valor nulo, en caso que así sea asigna el valor `True`, en caso contrario se coloca el valor `False`."
      ],
      "metadata": {
        "id": "hvPOEUsbstVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull()"
      ],
      "metadata": {
        "id": "_-74WtvwsqEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego, podemos utilizar la función `sum`, que nos permite sumar los valores que son `True` en cada columna."
      ],
      "metadata": {
        "id": "pu1b9n7mR5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "2JyegUTYs17R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso no tenemos valores nulos, pero si los tuviéramos es importante que sean reemplazados por algún valor. \n",
        "\n",
        "Ahora vamos a analizar nuestro **atributo objetivo**. Este atributo corresponde al valor que nos gustaría predecir a partir de los demás atributos (o parte de ellos). En nuestro caso utilizaremos la columna  *cardiovascular_disease*. Accedamos a ella para ver la naturaleza de los datos."
      ],
      "metadata": {
        "id": "6haIf2cDto4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"cardiovascular_disease\"]"
      ],
      "metadata": {
        "id": "iRVZG_hKttdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al parecer se trata de un atributo con valores 0's y 1's, donde 0 correspondería a que el paciente no presenta enfermedad cardiovascular, mientras que 1 significa que si tiene. Para comprobar esto, utilizamos la función `set` que nos permite ver todos los valores distintos que puede tomar este atributo."
      ],
      "metadata": {
        "id": "_UCz9MKqSDUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set(df[\"cardiovascular_disease\"])"
      ],
      "metadata": {
        "id": "6jUTDrTMuCwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con este resultados podemos concluir que trabajaremos con una clasificación binaria 👀. Otro paso fundamental, es contar la frecuencia de cada uno de los posibles valores pertenecientes a nuestro atributo/variable objetivo. Esto se realiza con la función `value_counts`."
      ],
      "metadata": {
        "id": "fFAkiosDSJ_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"cardiovascular_disease\"].value_counts()"
      ],
      "metadata": {
        "id": "PVaXCrXLt3jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que los datos están prácticamente balanceados; 164 pacientes presentan una enfermedad cardiovascular, mientras que 137 no tienen. En caso que exista un desbalance, hay que aplicar técnicas especiales de balanceo que veremos más adelante en el curso. Ahora sigamos analizando nuestros datos viendo un ejemplo."
      ],
      "metadata": {
        "id": "cgze1JsWuWkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(1)"
      ],
      "metadata": {
        "id": "uiOYyt7GuwEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que las columnas *chest_pain_type* y *thalassemia* son atributos con valores de tipo string (palabras). En aprendizaje de máquinas nuestros algoritmos o sistemas están basados en una serie de operaciones matemáticas sobre los datos, lo cual nos permite entrenar y luego realizar predicciones. Debido a esto, los modelos no pueden trabajar directamente con texto y estos deben ser transformados a números.\n",
        "\n",
        "La manera más sencilla de realizar esto es utilizando una transformación **One-Hot**. Esto significa que si tenemos un atributo de tipo string, el cual tiene `n` valores distintos, crearemos un total de `n` columnas nuevas con valores numéricos y binarios. Para ejemplificar, si se tiene una columna con los siguientes valores\n",
        "\n",
        "\n",
        "| chest_pain_type |\n",
        "|--------|\n",
        "| asymptomatic |\n",
        "| non_anginal_pain   |\n",
        "| typical_angina|\n",
        "| atypical_angina |\n",
        "\n",
        "y se transforma utilizando One-Hot Encoding, se generan las siguientes asignaciones columnas:\n",
        "\n",
        "\n",
        "| chest_pain_type_asymptomatic | chest_pain_type_non_anginal_pain | chest_pain_type_typical_angina | chest_pain_type_atypical_angina |\n",
        "|------------------------------|----------------------------------|--------------------------------|---------------------------------|\n",
        "| 1                            | 0                                | 0                              | 0                               |\n",
        "| 0                            | 1                                | 0                              | 0                               |\n",
        "| 0                            | 0                                | 1                              | 0                               |\n",
        "| 0                            | 0                                | 0                              | 1                               |\n",
        "\n",
        "\n",
        "Así, ya no tenemos que lidiar con valores no numéricos, y la cantidad de características aumenta. Para realizar esta transformación podemos utilizar la función `get_dummies`."
      ],
      "metadata": {
        "id": "g5wLI595vM9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df)\n",
        "df.sample(3)"
      ],
      "metadata": {
        "id": "x1pkmruDwTgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un dato que puede ser útil a la hora de analizar datasets, es que los dataframes de pandas nos permiten realizar consultas sobre nuestro dataset con los filtros. Por ejemplo, con el siguiente comando consultamos cuántas personas menores a 40 años presentan una enfermedad cardiovascular."
      ],
      "metadata": {
        "id": "XL8YdWb8T_-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df[\"age\"] <= 40) & (df[\"cardiovascular_disease\"] == 1)].shape[0]"
      ],
      "metadata": {
        "id": "Mr7bGlZYTjc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo siguiente que podemos hacer, es crear diferentes visualizaciones que nos permitan entender mejor nuestros datos. \n",
        "\n",
        "Por ejemplo, si utilizamos la función *box plot* podemos visualizar si es que existen datos alejados del promedio o con un extraño comportamiento en nuestro dataset, más conocidos como outliers."
      ],
      "metadata": {
        "id": "BEw9DVw-Uk4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[['age', 'cholesterol','resting_blood_pressure']].plot.box(vert = False, grid = True)"
      ],
      "metadata": {
        "id": "I7vw6acuUr6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "También podemos utilizar un scatter plot para ver si dos variables tienen una relación lineal o algún otro comportamiento"
      ],
      "metadata": {
        "id": "OC6UjVaYVlVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df['age'], df['cardiovascular_disease'])\n",
        "plt.xlabel('Edad')\n",
        "plt.ylabel('Enfermedad Cardiovascular')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pFzY5_CTVpdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la función `corr` visualizamos la matriz de correlación entre nuestros atributos"
      ],
      "metadata": {
        "id": "x6fgPym9WIZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr = df.corr()\n",
        "features = corr.index\n",
        "plt.figure(figsize=(15,15))\n",
        "ax = sns.heatmap(df[features].corr(), annot=True)"
      ],
      "metadata": {
        "id": "njUJvBeCWGwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamiento de modelos**"
      ],
      "metadata": {
        "id": "266x1MIjzglM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para poder entrenar nuestros modelos de aprendizaje supervisado debemos separar nuestros datos identificando claramente cuál será nuestra variable objetivo, es decir, la que queremos predecir. \n",
        "\n",
        "Generalmente se utiliza la variable `X` para los atributos de entrada o features, mientras que la variable `y` es el objetivo."
      ],
      "metadata": {
        "id": "LZ51pRHeXYk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(\"cardiovascular_disease\", axis=1) # Drop elimina la columna que se especifique, y axis = 1 significa eliminar valores de las columnas"
      ],
      "metadata": {
        "id": "cqOtWGnCXiBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['cardiovascular_disease']"
      ],
      "metadata": {
        "id": "DqpOMUOyXnW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teniendo ambos vectores, ya podemos entrenar modelos sobre nuestros datos 😄. En particular, veremos 3 modelos: Support Vector Machines (SVM), Decision Tree Classifier (DTC), y Randon Forest (RF).\n"
      ],
      "metadata": {
        "id": "nc74_GWvX1X6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Support Vector Classification (SVC)**\n",
        "\n"
      ],
      "metadata": {
        "id": "bQiafoy-cwg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una explicación muy general sobre este modelo, es que representa a los puntos de muestra en el espacio, separando las clases a 2 espacios lo más amplios posibles mediante un hiperplano de separación. Cuando llega un nuevo ejemplo, se analiza a cuál de los dos espacios pertenece para realizar la clasificación. En general, este algoritmo funciona muy bien con datos que son linealmente separables. En caso que no sea así, este algoritmo puede trasladar el problema de un problema no separable linealmente a uno que sí lo es. Veamos como podemos programar un modelo SVC."
      ],
      "metadata": {
        "id": "yvJfSL9bHqBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "nOmOjFUcc3i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc_model = SVC() # Creamos un objeto de tipo SVC.\n",
        "svc_model.fit(X, y) # Entrenamos el modelo con nuestros vectores X e y"
      ],
      "metadata": {
        "id": "KgQhQhD5X8wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego de que el modelo fue entrenado, podemos hacer  predicciones sobre algún conjunto de datos para así analizar el rendimiento de nuestro clasificador. Este proceso se realiza mediante la función `predict`."
      ],
      "metadata": {
        "id": "llWRVYegvdxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = svc_model.predict(X)"
      ],
      "metadata": {
        "id": "87GyzdhGXyqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para evaluar el rendimiento de nuestro modelo utilizamos la función `accuracy_score`. Esta función nos permite obtener la proporción de ejemplos que fueron correctamente clasificados sobre el total."
      ],
      "metadata": {
        "id": "v9lYyPT4v6L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vPhWTKS5c9k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'El accuracy obtenido por el modelo SVC es: {np.round(accuracy_score(y, predictions), 2)}')"
      ],
      "metadata": {
        "id": "bCtQlYZJYU4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y listo, hemos entrenado nuestro clasificador y hemos obtenido resultados bastante altos dada la complejidad del problema 🥳 \n",
        "\n"
      ],
      "metadata": {
        "id": "FwXv_i_GYYRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fired!](https://media3.giphy.com/media/3EAYL7KCtZJOJGtli6/giphy.gif?cid=ecf05e47b8ucx71fgr2bny9bn1mf8osjbh2z2grz4c4v4pyd&rid=giphy.gif&ct=g)"
      ],
      "metadata": {
        "id": "dzfnsSTNZ1pV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin embargo, hemos entrenamos el modelo sobre el conjunto de datos `X` e `y` y luego medimos el rendimiento sobre el mismo conjunto `y` original, es decir, no estamos midiendo correctamente el rendimiento del modelo en datos nuevos. Es como si llegaran 10 pacientes a la consulta, aprendemos cuál de ellos tiene o no tiene una enfermedad vascular. Y luego, evaluamos que tan bien clasificamos a los mismos 10 pacientes si tienen o no tienen la enfermedad. Esto no sólo es hacer trampa, sino que nos lleva al problema de **generalización** en aprendizaje de máquinas.\n",
        "\n",
        "¿Qué es la generalización?\n",
        "\n",
        "Supongamos que ustedes son unos estudiantes de primer año de universidad. Para poder estudiar para su primer control recolectan los controles realizados en toda la historia del ramo, y con una gran memoria se aprenden de memoria todas las preguntas y respuestas. \n",
        "\n",
        "Si al momento de rendir la prueba, aparece alguna de esas mismas preguntas, ustedes obtendrán una buena nota ya que saben la respuesta. Sin embargo, si por alguna razón el profesor decide innovar y crea una nueva pregunta, la nota que obtendrán probablemente será mala, ya que no estaban preparados para una nueva pregunta, un nuevo dato. Eso es lo que ocurre acá, no sabemos si el modelo es capaz de obtener buenos resultados en nuevos datos. \n",
        "\n",
        "¿Cómo podemos solucionar esto?\n",
        "\n",
        "Es importante antes de entrenar un modelo dividir nuestros datos en dos conjuntos; entrenamiento y testeo. El primer conjunto nos servirá para entrenar nuestro modelo, mientras que el segundo nos servirá para medir el rendimiento en datos que con alta probabilidad no fueron vistos anteriormente. Esto nos permite medir el nivel de **generalización** de los modelos.\n",
        "\n",
        "Más adelante veremos que en la práctica es mejor dividir el conjunto original en 3 particiones; entrenamiento, validación y testeo. Pero por ahora, nos quedaremos con las dos mencionadas. Para generar estos conjuntos lo hacemos con el siguiente código:"
      ],
      "metadata": {
        "id": "Evcn7evTYn93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "xI4PwBWAfhRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
      ],
      "metadata": {
        "id": "PF-Smf6SYhS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego, creamos y entrenamos nuestro modelo sobre el conjunto de entrenamiento."
      ],
      "metadata": {
        "id": "N6vKcUJedB2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svc_model = SVC()\n",
        "svc_model.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "WNFvBMfAZ938"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos las predicciones sobre el conjunto de test y calculamos el rendimiento."
      ],
      "metadata": {
        "id": "Ibt3z0CYdGy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = svc_model.predict(X_test)"
      ],
      "metadata": {
        "id": "qBJmaHtGaATO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'El accuracy obtenido por el modelo SVC es: {np.round(accuracy_score(Y_test, predictions), 2)}')"
      ],
      "metadata": {
        "id": "amDYkqOJaHRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos obtener los indices de los datos que nos permiten crear el hiperplano\n",
        "support_vector_indices = svc_model.support_\n",
        "print(support_vector_indices)"
      ],
      "metadata": {
        "id": "x9zgzGtrKmrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que estamos trabajando el múltiples dimensiones no es posible generar un gráfico adecuado, sin embargo, si tuvieramos un problema de clasificación binaria con sólo dos atributos de entrada, podríamos generar visualizaciones como la siguiente. Este método es diferente de una Regresión logistica, ya que busca encontrar la máxima separación entre los conjuntos, no una sóla recta."
      ],
      "metadata": {
        "id": "mWjiUgRvLkuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image info](https://github.com/christianversloot/machine-learning-articles/raw/main/images/support_vectors.png)"
      ],
      "metadata": {
        "id": "AP8Wuq45L2f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decission Tree Classifier**"
      ],
      "metadata": {
        "id": "sYrcSDBGwgAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Método de aprendizaje supervisado que permite resolver tareas de regresión y clasificación. Se llama árbol ya que visualmente parece un árbol invertido, y está compuesto tanto por una raíz, ramas y hojas como veremos más adelante. "
      ],
      "metadata": {
        "id": "ZNGIGR0gohOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree"
      ],
      "metadata": {
        "id": "0M_ZfQQIwwmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree = DecisionTreeClassifier( # Instanciamos nuestro árbol de decisión.\n",
        "    max_depth=3, # Forzamos que nuestro árbol sólo tenga 3 niveles de profundidad.\n",
        "    random_state = 11\n",
        "    )\n",
        "tree.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "dXfDusyuwisx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_predictions = tree.predict(X_test)"
      ],
      "metadata": {
        "id": "KEWozp9Qw34m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'El accuracy obtenido por el modelo Decision Tree es: {np.round(accuracy_score(Y_test, tree_predictions), 2)}')"
      ],
      "metadata": {
        "id": "WzcvXmaaw9Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una de las ventajas más importantes de los árboles de decisión es la transparencia y explicabilidad del proceso de predicción. Podemos conocer claramente todas las decisiones tomadas por el clasificador y visualizarla en un árbol de decisión."
      ],
      "metadata": {
        "id": "sgy-CBPZxM5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "plot_tree( # Función que nos permite visualizar el árbol de decisión ajustado.\n",
        "    tree,\n",
        "    class_names = [\"healthy\",\"sick\"],\n",
        "    feature_names = df.columns,\n",
        "    filled=True,\n",
        "    fontsize=11,\n",
        "    proportion = True,\n",
        "    label = \"all\"\n",
        "    \n",
        "    \n",
        "    )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WERHy5XxxIFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se ve en la figura anterior, este método evalua todas las variables de entrada para seleccionar la mejor salida. La variable más significativa se encontrará en la base del árbol creando una condición, a partir de esto se generan n posibilidades dependiendo la cantidad de posibles valores de la variable objetivo, luego se pasa al siguiente nodo y se repite el proceso, hasta llegar a una hoja."
      ],
      "metadata": {
        "id": "USveMliqpkfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para evaluar el nivel de importancia que tienen las caracterñisticas, los algoritmos basados en árboles nos ofrecen la importancia de cada variable al calcular normalmente el Mean Decrease Gini, pueden encontrar más información acá https://medium.com/the-artificial-impostor/feature-importance-measures-for-tree-models-part-i-47f187c1a2c3"
      ],
      "metadata": {
        "id": "WgBaT-m9x4aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_vil = pd.DataFrame(list(zip(df.columns,tree.feature_importances_)),\n",
        "             columns=[\"feature\",\"importance\"]\n",
        "            ).set_index(\"feature\")\n",
        "tree_vil.sort_values(\"importance\",ascending=False)"
      ],
      "metadata": {
        "id": "0ET4f-1kxeqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uno de los hiperparámetros de los árboles de decisión es la profundidad del mismo. Evaluemos cómo se comporta su rendimiento al cambiar la profundidad del árbol."
      ],
      "metadata": {
        "id": "vRIL8ap8yOgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depths = range(1,10)\n",
        "performances = []\n",
        "for depth in depths:\n",
        "    current_tree = DecisionTreeClassifier( # Instanciamos nuestro árbol de decisión.\n",
        "    max_depth=depth, # Forzamos que nuestro árbol sólo tenga 3 niveles de profundidad.\n",
        "    random_state = 11\n",
        "    )\n",
        "    current_tree.fit(X_train, Y_train)\n",
        "    predictions = current_tree.predict(X_test)\n",
        "\n",
        "    performances.append(accuracy_score(Y_test, predictions))"
      ],
      "metadata": {
        "id": "Jy5tAp5JyDpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(\n",
        "    depths,\n",
        "    performances\n",
        ")\n",
        "plt.xlabel(\"max_width\")\n",
        "plt.ylabel(\"mean_roc_auc_score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "64IzN_9xyWYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest Classifier**"
      ],
      "metadata": {
        "id": "2Rid_vFdzYwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Método basado en el modelo anterior. La idea es que en vez de tener un único árbol, existirán múltiples, de ahí viene el término forest. Dado que cada árbol entrega una clasificación, la clasificación quedará determinada por el valor que contenga más votos. Entre más árboles haya, más robusto y preciso será el algoritmo."
      ],
      "metadata": {
        "id": "hz1XcH_QptYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "Avh9DrwczgD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "PDZBOQaxzald"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_predictions = rf.predict(X_test)"
      ],
      "metadata": {
        "id": "Nn7gsg3UzmBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(Y_test, rf_predictions)"
      ],
      "metadata": {
        "id": "aKPBXoU0zmUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_vil = pd.DataFrame(list(zip(df.columns,rf.feature_importances_)),\n",
        "             columns=[\"feature\",\"importance\"]\n",
        "            ).set_index(\"feature\")\n",
        "rf_vil.sort_values(\"importance\",ascending=False)"
      ],
      "metadata": {
        "id": "zb7-BlyFzsf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_tree = rf.estimators_[-3]"
      ],
      "metadata": {
        "id": "CE_h7qj8zwde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "plot_tree( # Función que nos permite visualizar el árbol de decisión ajustado.\n",
        "    random_tree, # Objeto de nuestro árbol de decisión entrenado.\n",
        "    feature_names = df.columns, # Nombres de las variables utilizadas para entrenar.\n",
        "    class_names = [\"healthy\",\"sick\"], # Nombre de las clases que estamos prediciendo.\n",
        "    label = \"all\", # Etiquetamos todas características de cada nodo.\n",
        "    proportion = True, # Visualizamos las proporciones de datos en cada nodo de decisión,\n",
        "    filled=True, # Coloreamos los nodos\n",
        "    fontsize=11, # Establecemos el tamaño de la letra del texto dentro de cada nodo.,\n",
        "    max_depth=3 # Profundidad máxima del árbol\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kq3i1KdVzzJZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}